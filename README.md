LangChain Demo with Ollama (Gemma 2B)

This project is a simple Streamlit web application that demonstrates how to use LangChain with a locally running Ollama model (Gemma 2B).
Users can enter a question in the UI, and the app generates a response using the Gemma model via Ollama.
The project is designed for beginners who want to understand how LangChain, Streamlit, and local LLMs work together.

Features:

The application provides a minimal but complete LangChain pipeline.
It uses a systemâ€“user prompt structure, runs inference locally using Ollama, and displays the output in a Streamlit interface.
LangSmith tracing is also enabled for monitoring and debugging chains.

Tech Stack

1.Python

2.LangChain

3.Ollama

4.Streamlit

5.Gemma 2B (local LLM)
